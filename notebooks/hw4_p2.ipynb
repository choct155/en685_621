{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Problem 2\n",
    "\n",
    "**Part A [30 points]** *Note this is a Collaborative Problem*\n",
    "\n",
    "Using the Gaussian kernel develop psuedo code to create a SVM system to accomplish the\n",
    "following steps:\n",
    "\n",
    "+ Develop the ability to read in data xn with n observations and D dimensions (number of features).\n",
    "+ Develop the ability to randomly remove 20% of the observations per class and assign the observations as test data with the remaining 80% of the observations as training data.\n",
    "+ Using the equations in the Machine Learning I document under the Support Vector Machine section to develop an algorithm to process an input observations and compare it with the training observations.\n",
    "+ Expand the development to handle multiple classes. The SVM is a two class classifier so it is recommended to use one class vs. the others. This will require multiple models to be developed based on the number of classes.\n",
    "\n",
    "**Part B [20 points]** *Note this is a Collaborative Problem*\n",
    "+ Calculate the running time of the system above in O-notation.\n",
    "+ Calculate the total running time of the above system as T(n) with each line of pseudocode or code accounted for.\n",
    "+ How does the total running time T(n) compare to the running time in O-notation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load\n",
    "\n",
    "In all operations in which I read in Iris data for this course, I have leveraged the `load(0)` function of my [`Reader`](https://github.com/choct155/en685_621/blob/master/algorithms/iris/Reader.py) class. The function just leverages the fact that scikit-learn already has the Iris dataset, so the load occurs in constant time. The remaining work simply involves splitting the data for downstream use.\n",
    "\n",
    "```python\n",
    "class IrisReader:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load(self):\n",
    "        iris_in: np.array = datasets.load_iris()\n",
    "        self.data = dict(\n",
    "            setosa = iris_in[\"data\"][:50],\n",
    "            versicolor = iris_in[\"data\"][50:100],\n",
    "            virginica = iris_in[\"data\"][100:]\n",
    "        )\n",
    "```\n",
    "\n",
    "In effect, this function consists of two constant time operations for our purposes, yielding a recurrence of $2T(1)$ and $O(1)$ asymptotics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setosa': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2]]), 'versicolor': array([[7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3]]), 'virginica': array([[6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]])}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as skl\n",
    "from sklearn.svm import SVC\n",
    "from algorithms.iris.Reader import IrisReader\n",
    "from algorithms.iris.IrisOps import IrisOps\n",
    "from algorithms.classifiers.svm import SupportVector\n",
    "from typing import Dict, Tuple, Callable, List\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "iris_reader: IrisReader = IrisReader()\n",
    "iris_reader.load()\n",
    "raw_data: Dict[str, np.array] = iris_reader.data\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Test (20%) - Train (80%)\n",
    "\n",
    "While **`scikit-learn`** does have it's own train-test-split function, the shape of my data after the load does not precisely play nice with it. For this reason, I have another `test_train_split()` function defined within my [`IrisOps`](https://github.com/choct155/en685_621/blob/master/algorithms/iris/IrisOps.py) class. It combines the output of `Reader.load()` across classes, randomly permutes the data, and then yields a tuple containing the train and test sets, respectively. It also labels the data with an index value corresponding to each class.\n",
    "\n",
    "```python\n",
    "def test_train_split(raw_data: Dict[str, np.array], labels: List[str], train_prop: float) -> Tuple[np.array, np.array]:\n",
    "    \n",
    "    def process_label_group(data: np.array, idx: int) -> np.array:\n",
    "        n: int = len(data)\n",
    "        lab_idx: np.array = np.repeat(idx, n).reshape(n, 1)\n",
    "        return np.concatenate([lab_idx, data], axis=1)\n",
    "\n",
    "    print(\"Label Mapping: \", list(enumerate(labels)))\n",
    "    data: np.array = np.concatenate(list(\n",
    "        map(lambda lab: process_label_group(raw_data[lab[1]], lab[0]), enumerate(labels))\n",
    "    ))\n",
    "    permuted: np.array = np.random.permutation(data)\n",
    "\n",
    "    train_n: int = int(len(permuted) * 0.8)\n",
    "    return (permuted[:train_n], permuted[train_n:])\n",
    "```\n",
    "\n",
    "Assigning a label (see the helper `process_label_group()`) involves an assignment ($T(1)$), the instantiation of an array (\\~ $T(n)$), and the columnwise concatenation of two equivalently long arrays (\\~$T(n)$). We can ignore the reshaping of the array, which is effectively a metadata exercise. The helper function has a total cost of $2T(n) + 1$ if $n$ denotes the size of the array that is labeled. \n",
    "\n",
    "However, we should note that the `process_label_group()` only operates on a third of the data at a time in the Iris case, but since it still must do all three, the recurrence should hold. Once all three species have been labeled, they must be concatenated. When I have written concatenation operations in the past, I tend to use a fold with the first array as the starting value and appending additional value with each subsequent recursive call. So, a recurrence of $T(\\frac{2n}{3})$ seems fitting here.\n",
    "\n",
    "The next step is a permutation which likely involves sampling (indices) without replacement and then a sort. Let us assume we can deplete a collection in linear time and assume a sort reminiscient of merge-sort ($2T(\\frac{n}{2}) + O(n)$) for a total run time of $2T(\\frac{n}{2}) + 2T(n)$. \n",
    "\n",
    "Since we are dealing with arrays (which we will assume aren't Lists, or Vectors, or something under the hood), we should have constant time indexing which I suspect supports constant time splits. So, our last operation, splitting the data into the test and train sets adds $T(1)$.\n",
    "\n",
    "Altogether, the helper function, concatenation, permutation, and split have th following cost:\n",
    "\n",
    "\\begin{align}\n",
    "    T(n) &= 2T(n) + 1 + T(\\frac{2n}{3}) + 2T(\\frac{n}{2}) + 2T(n) + 1 \\\\\n",
    "    &= 4T(n) + T(\\frac{2n}{3}) + 2T(\\frac{n}{2}) + 2\n",
    "\\end{align}\n",
    "\n",
    "Since none of these terms are interacting with each other in a nested loop, the asymptotic growth is capped at $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Develop a Support Vector Machine Classifier with a Gaussian Kernel\n",
    "\n",
    "*Note: I initially set out to construct the full implementation from scratch, but I found I got a little hung up on the optimization portion. In the interests of time, I opted for using the `scikit-learn` implementation. I'll include the failed implementation in an Appendix to this notebook, just in case your curiosity gets the better of you. I have no expectation, but if you do feel inclined to provide a pointer or two, I wouldn't be upset about it.*\n",
    "\n",
    "The estimator used in this notebook is provided by the `SupportVector` class in my [`svm`](https://github.com/choct155/en685_621/blob/master/algorithms/classifiers/svm.py) module. It leverages the `scikit-learn` classifier under the hood, which is capable of multi-class assignment. However, to adhere to the spirit of the assignment, this class leverages that implementation as a binary classifier, and then uses a round robin approach to collect labels which are then reconciled at the end.\n",
    "\n",
    "### SVM Psuedo-Code Analysis\n",
    "\n",
    "Since a stock approach is leveraged below, let's evaluate a psuedo-code version of the SVM approach. The focus here is on the dual soft margin SVM because it exposes an isolated inner product for the feature matrix, which creates an opportunity for kernel trickery. For the dual soft margin SVM our task is to minimize the following relation with respect to $\\alpha$.\n",
    "\n",
    "\\begin{align}\n",
    "    L(\\alpha) &= \\Sigma_{n=1}^N - \\frac{1}{2} \\Sigma_{n=1}^N \\Sigma_{m=1}^N a_n a_m y_n y_m k(\\textbf{x_n}, \\textbf{x_m}) \\\\\n",
    "    &= \\alpha^T - \\frac{1}{2} \\alpha^T Q \\alpha \\\\\n",
    "\\end{align}\n",
    "where $Q_{N \\times N} = y_n^T y_m * K(\\textbf{x_n}, \\textbf{x_m})$ and subject to $0 \\leq \\alpha_n \\leq C \\hspace{3pt} \\forall \\hspace{3pt} n$ and $\\alpha^T y = 0$. The matrix $K(\\textbf{x_n}, \\textbf{x_m})$ is commonly known as the Gram Matrix, and is the insertion point for the gaussian kernel (or any other kernel we might choose). As discussed in Problem 1, the construction of the Gram Matrix costs a running time of $T(mn) + 3$ with an asymptotic cost of $O(mn)$ where $m$ is the number of test observations and $n$ is the number of train observations. In this instance, we are actually calculating the inner product of the train data with itself, so we are looking at $T(n^2) + 3$ and $O(n^2)$.\n",
    "\n",
    "The problem can mostly be split in two: setting up for the optimization used to discover the weights and bias, and then solving the optimization. The last part, label assignment, is just an $T(n)$ map over the test data that leverages the weights and bias identified by optimization.\n",
    "\n",
    "#### Set Up\n",
    "\n",
    "1. Compute the Gram Matrix ($T(n^2) + T(3)$)\n",
    "2. Compute inner product of labels and use it to scale the Gram Matrix ($T(n) + T(1)$)\n",
    "3. Construct initial guess for matrix of Lagrangian multipliers $\\alpha_n$ ($T(1)$)\n",
    "4. Construct bounds for the inequality constraints by concatenating two vectors of length $n$, populated with values of 0 and $C$, respectively. Additionally, concatenate two matrices to facilitate conformal comparison to the bounds, which is more costly if simple appends are used. ($T(n) + T(n^2)$; see problem 1 for discussion of concatenation)\n",
    "5. Assign existing labels for the equality constraint ($T(1)$), and the value of zero ($T(1)$)\n",
    "\n",
    "Total set up cost is $2T(n^2) + 2T(n) + T(7)$.\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "I am a bit unclear about how to appraise the running cost of a non-deterministic algorithm like stochastic gradient descent, or even one that is so sensitive to starting conditions like base gradient descent. It strikes me that I can at least say a computational challenge is the need to solve the likelihood at each step for all dimensions of the data to determine the direction of greatest ascent (in order to move in the opposite direction). That is, optimization scales at $O(d)$ where $d$ is the dimensionality of the data to be classified.\n",
    "\n",
    "The total running cost of set up, \"optimization\", and labeling is therefore $2T(n^2) + 3T(n) + T(7) + O(d)$. When looking at the asymptotic cost, it is unclear whether or not the observation count or the dimensionality count will be dominant in all cases, so we can cover both bases with $O(n^2) + O(d)$. Running time again exceeds asymptotic time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Multi-Class Assignment via Binary Classification\n",
    "\n",
    "The approach taken in the `SupportVector` class is as follows:\n",
    "\n",
    "For each class: \n",
    "1. Alter the labels to assign +1 to the selected class and -1 to the other two in the train data\n",
    "2. Train SVM classifier on train data\n",
    "3. Predict labels on test data\n",
    "4. Assign the selected class to labels with values of +1, and \"other\" to labels with values of -1\n",
    "\n",
    "Once each class has a vector of labels, filter out \"other\" from each row and compare to the truth vector. If the match is exact, it counts as a match. If there are conflicting matches across runs, it counts as a mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:  [(0, 'setosa'), (1, 'versicolor'), (2, 'virginica')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.5)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = SupportVector(raw_data)\n",
    "rbf = sv.fit([1,2,3,4], sv.train)\n",
    "rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0,\n",
       " 'predictions': [array(['versicolor'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U10'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U10')],\n",
       " 'truth': [array(['versicolor'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['setosa'], dtype='<U6'),\n",
       "  array(['versicolor'], dtype='<U10'),\n",
       "  array(['virginica'], dtype='<U9'),\n",
       "  array(['virginica'], dtype='<U9')]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = SupportVector.binary_round_robin(sv.labels, [1,2,3,4], sv.train, sv.test)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, there have been no conflicting labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Appendix - Stalled Attempt at Ground Up Implementation (for now)\n",
    "\n",
    "For the dual soft margin SVM our task is to minimize the following relation with respect to $\\alpha$.\n",
    "\n",
    "\\begin{align}\n",
    "    L(\\alpha) &= \\Sigma_{n=1}^N - \\frac{1}{2} \\Sigma_{n=1}^N \\Sigma_{m=1}^N a_n a_m y_n y_m k(\\textbf{x_n}, \\textbf{x_m}) \\\\\n",
    "    &= \\alpha^T - \\frac{1}{2} \\alpha^T Q \\alpha \\\\\n",
    "\\end{align}\n",
    "where $Q_{N \\times N} = y_n^T y_m * K(\\textbf{x_n}, \\textbf{x_m})$ and subject to $0 \\leq \\alpha_n \\leq C \\hspace{3pt} \\forall \\hspace{3pt} n$ and $\\alpha^T y = 0$. The matrix $K(\\textbf{x_n}, \\textbf{x_m})$ is commonly known as the Gram Matrix, and is the insertion point for the gaussian kernel (or any other kernel we might choose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(X: np.array, Y: np.array, spread: float) -> np.array:\n",
    "    x_rows, x_cols = X.shape\n",
    "    y_rows, y_cols = Y.shape\n",
    "    out: np.array = np.zeros((x_rows, y_rows))\n",
    "    \n",
    "    def g(x_0: np.array, x_n: np.array) -> float:\n",
    "        normalization: float = 1 / ((np.sqrt(2*np.pi)*spread)**len(x_0))\n",
    "        distance: float = (x_0-x_n).dot((x_0-x_n))\n",
    "        exponential: float = np.exp((-0.5/spread**2) * distance)\n",
    "        return normalization * exponential\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        for j, y in enumerate(Y):\n",
    "            out[i, j] = g(x, y)\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal solution of $\\alpha$ is actually a quadratic programming problem, so we will use a solver from an optimization library called [CVXOPT](https://cvxopt.org/userguide/intro.html). Specifically, we will use the [`qp`](https://cvxopt.org/userguide/coneprog.html#quadratic-programming) solver because it is designed for such problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://cvxopt.org/userguide/coneprog.html#quadratic-programming\" width=\"1200\" height=\"1000\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://cvxopt.org/userguide/coneprog.html#quadratic-programming\" width=\"1200\" height=\"1000\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what we are doing here, let's walk through each of the arguments and see how they correspond.\n",
    "\n",
    "+ `x` is the parameter to be optimized over, so it is $\\alpha$\n",
    "+ `P` is our scaled Gram Matrix $Q_{N \\times N} = y_n^T y_m * K(\\textbf{x_n}, \\textbf{x_m})$\n",
    "+ `q` appears to be our initial guess for $\\alpha$\n",
    "+ Our first constraint has to be reworked a bit to fit the shape of the API (see $Gx \\leq h$). Instead of expressing the bounds on $\\alpha$ as the interval $0 \\leq \\alpha_n \\leq C$, we will break it up into two expressions: $-\\alpha_n \\leq 0$ and $\\alpha_n \\leq C$. From here, we can stack the two composite constraints together.\n",
    "    + `h` is the vertical concatenation of two $N \\times 1$ vectors. The first is filled with zeros, capturing the lower bound of our initial constraint. The second is filled with values of $C$, the upper bound of the initial constraint. \n",
    "    + `G` is the cofficient matrix that enables us to compare each value of `x` (i.e. $\\alpha$) to the constraints. In this case, we just want a direct value comparison, so we will stack two identity matrices on top of one another. Note, that each corresponds to each half of our initial constraint, so we must multiply the upper identity matrix by -1 to test $-\\alpha_n \\leq 0$.\n",
    "+ `A` plays the role of the labels $y$ in the second constraint $\\alpha^T y = 0$, and `b` plays the role of the zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:  [(0, 'setosa'), (1, 'versicolor'), (2, 'virginica')]\n"
     ]
    }
   ],
   "source": [
    "def test_train_split(raw_data: Dict[str, np.array], labels: List[str], train_prop: float) -> Tuple[np.array, np.array]:\n",
    "    \n",
    "    def process_label_group(data: np.array, idx: int) -> np.array:\n",
    "        n: int = len(data)\n",
    "        lab_idx: np.array = np.repeat(idx, n).reshape(n, 1)\n",
    "        return np.concatenate([lab_idx, data], axis=1)\n",
    "    \n",
    "    print(\"Label Mapping: \", list(enumerate(labels)))\n",
    "    data: np.array = np.concatenate(list(\n",
    "        map(lambda lab: process_label_group(raw_data[lab[1]], lab[0]), enumerate(labels))\n",
    "    ))\n",
    "    permuted: np.array = np.random.permutation(data)\n",
    "    \n",
    "    train_n: int = int(len(permuted) * 0.8)\n",
    "    return (permuted[:train_n], permuted[train_n:])\n",
    "    \n",
    "train, test = test_train_split(raw_data, [\"setosa\", \"versicolor\", \"virginica\"], .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data: np.array, pos_y_class: int) -> Tuple[np.array, np.array]:\n",
    "    y: np.array = np.where(data[:,0] == pos_y_class, 1, -1)\n",
    "    x_in: np.array = data[:, 1:]\n",
    "    X: np.array = (x_in - x_in.mean(axis=0)) / x_in.std(axis=0)\n",
    "    return y, X\n",
    "    \n",
    "y_train, X_train = process_data(train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Rank(A) < p or Rank([P; A; G]) < n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArithmeticError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/tools/miniconda3/envs/ipy/lib/python3.8/site-packages/cvxopt/misc.py\u001b[0m in \u001b[0;36mfactor\u001b[0;34m(W, H, Df)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m                     \u001b[0mlapack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpotrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArithmeticError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArithmeticError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/tools/miniconda3/envs/ipy/lib/python3.8/site-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mconeqp\u001b[0;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrti\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rti'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2066\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mArithmeticError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/ipy/lib/python3.8/site-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mkktsolver\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m   1980\u001b[0m          \u001b[0;32mdef\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1981\u001b[0;31m              \u001b[0;32mreturn\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/ipy/lib/python3.8/site-packages/cvxopt/misc.py\u001b[0m in \u001b[0;36mfactor\u001b[0;34m(W, H, Df)\u001b[0m\n\u001b[1;32m   1443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                     \u001b[0mlapack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpotrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArithmeticError\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-d9d63fc323ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-d9d63fc323ca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(y, X, margin_error, bandwidth)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feastol'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     return solvers.qp(\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/ipy/lib/python3.8/site-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mqp\u001b[0;34m(P, q, G, h, A, b, solver, kktsolver, initvals, **kwargs)\u001b[0m\n\u001b[1;32m   4483\u001b[0m             'residual as dual infeasibility certificate': dinfres}\n\u001b[1;32m   4484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4485\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconeqp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkktsolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tools/miniconda3/envs/ipy/lib/python3.8/site-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mconeqp\u001b[0;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mArithmeticError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2067\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rank(A) < p or Rank([P; A; G]) < n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Rank(A) < p or Rank([P; A; G]) < n"
     ]
    }
   ],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "def train(y, X, margin_error, bandwidth):\n",
    "    n_rows, n_cols = X.shape\n",
    "    y_2d: np.array = y.reshape(1, n_rows).astype(float)\n",
    "    \n",
    "    P: np.array = -np.dot(y_2d.T, y_2d) * gram_matrix(X, X, bandwidth)\n",
    "    q: np.array = np.ones((n_rows, 1))\n",
    "    h: np.array = np.concatenate([\n",
    "        np.zeros((n_rows, 1)),\n",
    "        np.ones((n_rows, 1)) * margin_error\n",
    "    ], axis=0)\n",
    "    G: np.array = np.concatenate([\n",
    "        -np.eye((n_rows)),\n",
    "        np.eye((n_rows))\n",
    "    ], axis=0)\n",
    "    A: np.array = y_2d\n",
    "    b: np.array = np.zeros(1)\n",
    "        \n",
    "    for elem in [P, q, h, G, A, b]:\n",
    "        print(np.ndim(elem))\n",
    "        \n",
    "    solvers.options['abstol'] = 1e-10\n",
    "    solvers.options['reltol'] = 1e-10\n",
    "    solvers.options['feastol'] = 1e-10\n",
    "    \n",
    "    return solvers.qp(\n",
    "        matrix(P), \n",
    "        matrix(q), \n",
    "        matrix(G), \n",
    "        matrix(h), \n",
    "        matrix(A), \n",
    "        matrix(b)\n",
    "    )\n",
    "\n",
    "train(y_train, X_train, 10, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "??matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('ipy': conda)",
   "language": "python",
   "name": "python38064bitipyconda835fb490d1e04038aa80aa14f78d3b6f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
